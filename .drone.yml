# Copyright (c) 2020 SIGHUP s.r.l All rights reserved.
# Use of this source code is governed by a BSD-style
# license that can be found in the LICENSE file.

name: license
kind: pipeline
type: docker

steps:
  - name: check
    image: docker.io/library/golang:1.21
    pull: always
    commands:
      - go install github.com/google/addlicense@v1.1.1
      - addlicense -c "SIGHUP s.r.l" -v -l bsd --check .

---
name: policeman
kind: pipeline
type: docker
depends_on:
  - license

platform:
  os: linux
  arch: amd64

environment:
  # Mise configuration envvars.
  MISE_DATA_DIR: /mise-data
  MISE_OVERRIDE_CONFIG_FILENAMES: "mise.ci.toml"

steps:
  # - name: lint
  #   image: quay.io/sighup/policeman
  #   pull: always
  #   environment:
  #     FILTER_REGEX_EXCLUDE: (\.github)
  #     VALIDATE_TERRAFORM_TERRASCAN: "false"
  #     # Identifies false positives like missing 'selector'.
  #     # Doing this is valid for Kustomize patches
  #     VALIDATE_KUBERNETES_KUBEVAL: "false"
  #     # Some duplicated code is intended.
  #     VALIDATE_JSCPD: "false"
  #     VALIDATE_DOCKERFILE: "false"
  #     # Disable natural language checks
  #     VALIDATE_NATURAL_LANGUAGE: "false"
  #   depends_on:
  #     - clone

  - name: render
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    environment:
      GITHUB_TOKEN:
        from_secret: github_token
    volumes:
      - name: mise-cache
        path: /mise-data
    depends_on:
      - clone
    commands:
      - |
        mise use kustomize@5.6.0
        eval "$(mise activate bash --shims)"
      - kustomize build katalog/velero/velero-on-prem > velero.yml

  - name: check-deprecated-apis
    image: us-docker.pkg.dev/fairwinds-ops/oss/pluto:v5
    pull: always
    depends_on:
      - render
    commands:
      # we use --ignore-deprecations because we don't want the CI to fail when the API has not been removed yet.
      - /pluto detect velero.yml --target-versions=k8s=v1.33.0 --ignore-deprecations

volumes:
  - name: mise-cache
    host:
      path: /root/mise_data_dir


---
name: e2e-kubernetes-1.31
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

environment:
  # Mise configuration.
  MISE_DATA_DIR: "/mise-data"
  MISE_OVERRIDE_CONFIG_FILENAMES: "mise.ci.toml"

  # Cluster version.
  CLUSTER_VERSION: "v1.31.9"
  CLUSTER_NAME: "${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.9"

  # Default tool versions.
  KUBECTL_VERSION: "v1.31.9"
  JQ_VERSION: "1.8.1"
  BATS_VERSION: "1.1.0"
  KUSTOMIZE_VERSION: "5.6.0"
  VELERO_VERSION: "1.17.1"

  # /drone/src is the default workdir for the pipeline
  # using this folder we don't need to mount another
  # shared volume between the steps
  KUBECONFIG: "/drone/src/kubeconfig-131"

  # AWS configuration.
  AWS_CI_PIPELINE_NUMBER: "dr-131-aws"
  AWS_CLOUD_CREDENTIALS_PATH: "/drone/src/aws_cloud_credentials_131.yaml"
  AWS_BACKUP_STORAGE_LOCATION_PATH: "/drone/src/aws_backup_storage_location_131.yaml"

  # GCP configuration.
  GCP_CI_PIPELINE_NUMBER: "dr-131-gcp"
  GOOGLE_APPLICATION_CREDENTIALS: "/drone/src/gcp_terraform-credentials-131.json"
  GCP_CLOUD_CREDENTIALS_PATH: "/drone/src/gcp_cloud_credentials_131.yaml"
  GCP_BACKUP_STORAGE_LOCATION_PATH: "/drone/src/gcp_backup_storage_location_131.yaml"

steps:
  - name: create-kind-cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [clone]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    environment:
      # GITHUB_TOKEN must be defined in the step scope, otherwise it won't work.
      # drone exec will fail with a `cannot unmarshal !!map into string` error.
      GITHUB_TOKEN:
        from_secret: github_token
    commands:
      - |-
        mise use kind@$${KIND_VERSION} kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      # NOTE: kind's `--wait` flag that waits for the control-plane to be ready.
      # It does not work when disabling the default CNI. It will always go in timeout.
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      # Save the kubeconfig so we can use it in other steps.
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [create-kind-cluster]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-install.sh

  - name: test-backup-restore
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [test-install]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh
      - bats -t katalog/tests/velero/velero-file-system-backup.sh
      - bats -t katalog/tests/velero/velero-snapshot-backup.sh

  - name: init-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore]
    environment:
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${AWS_CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
      - terraform output -raw cloud_credentials > "$${AWS_CLOUD_CREDENTIALS_PATH}"
      - terraform output -raw backup_storage_location > "$${AWS_BACKUP_STORAGE_LOCATION_PATH}"

  - name: apply-aws-configuration
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [init-aws]
    commands:
      - |-
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kubectl apply -f "$${AWS_CLOUD_CREDENTIALS_PATH}" -n kube-system
      - kubectl apply -f "$${AWS_BACKUP_STORAGE_LOCATION_PATH}" -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - kubectl rollout status deploy/velero -n kube-system --timeout=180s
      - kubectl get pods -n kube-system

  - name: test-backup-restore-aws
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [apply-aws-configuration]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh

  - name: destroy-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore-aws]
    environment:
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${AWS_CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
    when:
      status:
        - success
        # - failure

  - name: init-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [destroy-aws]
    environment:
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - echo $${GCP_CREDENTIALS} > "$${GOOGLE_APPLICATION_CREDENTIALS}"
      - cd examples/gcp-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${GCP_CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${GCP_CI_PIPELINE_NUMBER}-${DRONE_BUILD_NUMBER}"
        --var="gcp_project=$${GCP_PROJECT}"
      - terraform output -raw cloud_credentials > "$${GCP_CLOUD_CREDENTIALS_PATH}"
      - terraform output -raw backup_storage_location > "$${GCP_BACKUP_STORAGE_LOCATION_PATH}"

  - name: apply-gcp-configuration
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [init-gcp]
    commands:
      - |-
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kubectl apply -f "$${GCP_CLOUD_CREDENTIALS_PATH}" -n kube-system
      - kubectl apply -f "$${GCP_BACKUP_STORAGE_LOCATION_PATH}" -n kube-system
      - kustomize build katalog/velero/velero-gcp | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - kubectl rollout status deploy/velero -n kube-system --timeout=180s
      - kubectl get pods -n kube-system

  - name: test-backup-restore-gcp
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [apply-gcp-configuration]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh

  - name: destroy-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore-gcp]
    environment:
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - cd examples/gcp-example
      - echo $${GCP_CREDENTIALS} > $${GOOGLE_APPLICATION_CREDENTIALS}
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${GCP_CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${GCP_CI_PIPELINE_NUMBER}-${DRONE_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
    when:
      status:
        - success
        - failure

  - name: delete-kind-cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [destroy-gcp]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - |
        mise use kind@$${KIND_VERSION}
        eval "$(mise activate bash --shims)"
      # does not matter if the command fails
      - kind delete cluster --name $${CLUSTER_NAME} || true
    when:
      status:
        - success
        # - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock
  - name: mise-cache
    host:
      path: /root/mise_data_dir

---
name: e2e-kubernetes-1.32
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

environment:
  # Mise configuration.
  MISE_DATA_DIR: "/mise-data"
  MISE_OVERRIDE_CONFIG_FILENAMES: "mise.ci.toml"

  # Cluster version.
  CLUSTER_VERSION: "v1.32.5"
  CLUSTER_NAME: "${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.32.5"

  # Default tool versions.
  KUBECTL_VERSION: "v1.32.5"
  JQ_VERSION: "1.8.1"
  BATS_VERSION: "1.1.0"
  KUSTOMIZE_VERSION: "5.6.0"
  VELERO_VERSION: "1.17.1"

  # /drone/src is the default workdir for the pipeline
  # using this folder we don't need to mount another
  # shared volume between the steps
  KUBECONFIG: "/drone/src/kubeconfig-132"

  # AWS configuration.
  AWS_CI_PIPELINE_NUMBER: "dr-132-aws"
  AWS_CLOUD_CREDENTIALS_PATH: "/drone/src/aws_cloud_credentials_132.yaml"
  AWS_BACKUP_STORAGE_LOCATION_PATH: "/drone/src/aws_backup_storage_location_132.yaml"

  # GCP configuration.
  GCP_CI_PIPELINE_NUMBER: "dr-132-gcp"
  GOOGLE_APPLICATION_CREDENTIALS: "/drone/src/gcp_terraform-credentials-132.json"
  GCP_CLOUD_CREDENTIALS_PATH: "/drone/src/gcp_cloud_credentials_132.yaml"
  GCP_BACKUP_STORAGE_LOCATION_PATH: "/drone/src/gcp_backup_storage_location_132.yaml"

steps:
  - name: create-kind-cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [clone]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    environment:
      # GITHUB_TOKEN must be defined in the step scope, otherwise it won't work.
      # drone exec will fail with a `cannot unmarshal !!map into string` error.
      GITHUB_TOKEN:
        from_secret: github_token
    commands:
      - |-
        mise use kind@$${KIND_VERSION} kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      # NOTE: kind's `--wait` flag that waits for the control-plane to be ready.
      # It does not work when disabling the default CNI. It will always go in timeout.
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      # Save the kubeconfig so we can use it in other steps.
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [create-kind-cluster]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-install.sh

  - name: test-backup-restore
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [test-install]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh
      - bats -t katalog/tests/velero/velero-file-system-backup.sh
      - bats -t katalog/tests/velero/velero-snapshot-backup.sh

  - name: init-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore]
    environment:
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${AWS_CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
      - terraform output -raw cloud_credentials > "$${AWS_CLOUD_CREDENTIALS_PATH}"
      - terraform output -raw backup_storage_location > "$${AWS_BACKUP_STORAGE_LOCATION_PATH}"

  - name: apply-aws-configuration
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [init-aws]
    commands:
      - |-
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kubectl apply -f "$${AWS_CLOUD_CREDENTIALS_PATH}" -n kube-system
      - kubectl apply -f "$${AWS_BACKUP_STORAGE_LOCATION_PATH}" -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - kubectl rollout status deploy/velero -n kube-system --timeout=180s
      - kubectl get pods -n kube-system

  - name: test-backup-restore-aws
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [apply-aws-configuration]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh

  - name: destroy-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore-aws]
    environment:
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${AWS_CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
    when:
      status:
        - success
        # - failure

  - name: init-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [destroy-aws]
    environment:
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - echo $${GCP_CREDENTIALS} > "$${GOOGLE_APPLICATION_CREDENTIALS}"
      - cd examples/gcp-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${GCP_CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${GCP_CI_PIPELINE_NUMBER}-${DRONE_BUILD_NUMBER}"
        --var="gcp_project=$${GCP_PROJECT}"
      - terraform output -raw cloud_credentials > "$${GCP_CLOUD_CREDENTIALS_PATH}"
      - terraform output -raw backup_storage_location > "$${GCP_BACKUP_STORAGE_LOCATION_PATH}"

  - name: apply-gcp-configuration
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [init-gcp]
    commands:
      - |-
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kubectl apply -f "$${GCP_CLOUD_CREDENTIALS_PATH}" -n kube-system
      - kubectl apply -f "$${GCP_BACKUP_STORAGE_LOCATION_PATH}" -n kube-system
      - kustomize build katalog/velero/velero-gcp | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - kubectl rollout status deploy/velero -n kube-system --timeout=180s
      - kubectl get pods -n kube-system

  - name: test-backup-restore-gcp
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [apply-gcp-configuration]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh

  - name: destroy-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore-gcp]
    environment:
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - cd examples/gcp-example
      - echo $${GCP_CREDENTIALS} > $${GOOGLE_APPLICATION_CREDENTIALS}
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${GCP_CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${GCP_CI_PIPELINE_NUMBER}-${DRONE_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
    when:
      status:
        - success
        - failure

  - name: delete-kind-cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [destroy-gcp]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - |
        mise use kind@$${KIND_VERSION}
        eval "$(mise activate bash --shims)"
      # does not matter if the command fails
      - kind delete cluster --name $${CLUSTER_NAME} || true
    when:
      status:
        - success
        # - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock
  - name: mise-cache
    host:
      path: /root/mise_data_dir

---
name: e2e-kubernetes-1.33
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

environment:
  # Mise configuration.
  MISE_DATA_DIR: "/mise-data"
  MISE_OVERRIDE_CONFIG_FILENAMES: "mise.ci.toml"

  # Cluster version.
  CLUSTER_VERSION: "v1.33.2"
  CLUSTER_NAME: "${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.33.2"

  # Default tool versions.
  KUBECTL_VERSION: "v1.33.2"
  JQ_VERSION: "1.8.1"
  BATS_VERSION: "1.1.0"
  KUSTOMIZE_VERSION: "5.6.0"
  VELERO_VERSION: "1.17.1"

  # /drone/src is the default workdir for the pipeline
  # using this folder we don't need to mount another
  # shared volume between the steps
  KUBECONFIG: "/drone/src/kubeconfig-133"

  # AWS configuration.
  AWS_CI_PIPELINE_NUMBER: "dr-133-aws"
  AWS_CLOUD_CREDENTIALS_PATH: "/drone/src/aws_cloud_credentials_133.yaml"
  AWS_BACKUP_STORAGE_LOCATION_PATH: "/drone/src/aws_backup_storage_location_133.yaml"

  # GCP configuration.
  GCP_CI_PIPELINE_NUMBER: "dr-133-gcp"
  GOOGLE_APPLICATION_CREDENTIALS: "/drone/src/gcp_terraform-credentials-133.json"
  GCP_CLOUD_CREDENTIALS_PATH: "/drone/src/gcp_cloud_credentials_133.yaml"
  GCP_BACKUP_STORAGE_LOCATION_PATH: "/drone/src/gcp_backup_storage_location_133.yaml"

steps:
  - name: create-kind-cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [clone]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    environment:
      # GITHUB_TOKEN must be defined in the step scope, otherwise it won't work.
      # drone exec will fail with a `cannot unmarshal !!map into string` error.
      GITHUB_TOKEN:
        from_secret: github_token
    commands:
      - |-
        mise use kind@$${KIND_VERSION} kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      # NOTE: kind's `--wait` flag that waits for the control-plane to be ready.
      # It does not work when disabling the default CNI. It will always go in timeout.
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      # Save the kubeconfig so we can use it in other steps.
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [create-kind-cluster]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-install.sh

  - name: test-backup-restore
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [test-install]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh
      - bats -t katalog/tests/velero/velero-file-system-backup.sh
      - bats -t katalog/tests/velero/velero-snapshot-backup.sh

  - name: init-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore]
    environment:
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${AWS_CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
      - terraform output -raw cloud_credentials > "$${AWS_CLOUD_CREDENTIALS_PATH}"
      - terraform output -raw backup_storage_location > "$${AWS_BACKUP_STORAGE_LOCATION_PATH}"

  - name: apply-aws-configuration
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [init-aws]
    commands:
      - |-
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kubectl apply -f "$${AWS_CLOUD_CREDENTIALS_PATH}" -n kube-system
      - kubectl apply -f "$${AWS_BACKUP_STORAGE_LOCATION_PATH}" -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - kubectl rollout status deploy/velero -n kube-system --timeout=180s
      - kubectl get pods -n kube-system

  - name: test-backup-restore-aws
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [apply-aws-configuration]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh

  - name: destroy-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore-aws]
    environment:
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${AWS_CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
    when:
      status:
        - success
        # - failure

  - name: init-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [destroy-aws]
    environment:
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - echo $${GCP_CREDENTIALS} > "$${GOOGLE_APPLICATION_CREDENTIALS}"
      - cd examples/gcp-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${GCP_CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${GCP_CI_PIPELINE_NUMBER}-${DRONE_BUILD_NUMBER}"
        --var="gcp_project=$${GCP_PROJECT}"
      - terraform output -raw cloud_credentials > "$${GCP_CLOUD_CREDENTIALS_PATH}"
      - terraform output -raw backup_storage_location > "$${GCP_BACKUP_STORAGE_LOCATION_PATH}"

  - name: apply-gcp-configuration
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [init-gcp]
    commands:
      - |-
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kubectl apply -f "$${GCP_CLOUD_CREDENTIALS_PATH}" -n kube-system
      - kubectl apply -f "$${GCP_BACKUP_STORAGE_LOCATION_PATH}" -n kube-system
      - kustomize build katalog/velero/velero-gcp | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - kubectl rollout status deploy/velero -n kube-system --timeout=180s
      - kubectl get pods -n kube-system

  - name: test-backup-restore-gcp
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [apply-gcp-configuration]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh

  - name: destroy-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore-gcp]
    environment:
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - cd examples/gcp-example
      - echo $${GCP_CREDENTIALS} > $${GOOGLE_APPLICATION_CREDENTIALS}
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${GCP_CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${GCP_CI_PIPELINE_NUMBER}-${DRONE_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
    when:
      status:
        - success
        - failure

  - name: delete-kind-cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [destroy-gcp]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - |
        mise use kind@$${KIND_VERSION}
        eval "$(mise activate bash --shims)"
      # does not matter if the command fails
      - kind delete cluster --name $${CLUSTER_NAME} || true
    when:
      status:
        - success
        # - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock
  - name: mise-cache
    host:
      path: /root/mise_data_dir

---
name: e2e-kubernetes-1.34
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

environment:
  # Mise configuration.
  MISE_DATA_DIR: "/mise-data"
  MISE_OVERRIDE_CONFIG_FILENAMES: "mise.ci.toml"

  # Cluster version.
  CLUSTER_VERSION: "v1.34.0"
  CLUSTER_NAME: "${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.34.0"

  # Default tool versions.
  KUBECTL_VERSION: "v1.34.0"
  JQ_VERSION: "1.8.1"
  BATS_VERSION: "1.1.0"
  KUSTOMIZE_VERSION: "5.6.0"
  VELERO_VERSION: "1.17.1"

  # /drone/src is the default workdir for the pipeline
  # using this folder we don't need to mount another
  # shared volume between the steps
  KUBECONFIG: "/drone/src/kubeconfig-133"

  # AWS configuration.
  AWS_CI_PIPELINE_NUMBER: "dr-134-aws"
  AWS_CLOUD_CREDENTIALS_PATH: "/drone/src/aws_cloud_credentials_134.yaml"
  AWS_BACKUP_STORAGE_LOCATION_PATH: "/drone/src/aws_backup_storage_location_134.yaml"

  # GCP configuration.
  GCP_CI_PIPELINE_NUMBER: "dr-134-gcp"
  GOOGLE_APPLICATION_CREDENTIALS: "/drone/src/gcp_terraform-credentials-134.json"
  GCP_CLOUD_CREDENTIALS_PATH: "/drone/src/gcp_cloud_credentials_134.yaml"
  GCP_BACKUP_STORAGE_LOCATION_PATH: "/drone/src/gcp_backup_storage_location_134.yaml"

steps:
  - name: create-kind-cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [clone]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    environment:
      # GITHUB_TOKEN must be defined in the step scope, otherwise it won't work.
      # drone exec will fail with a `cannot unmarshal !!map into string` error.
      GITHUB_TOKEN:
        from_secret: github_token
    commands:
      - |-
        mise use kind@$${KIND_VERSION} kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      # NOTE: kind's `--wait` flag that waits for the control-plane to be ready.
      # It does not work when disabling the default CNI. It will always go in timeout.
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      # Save the kubeconfig so we can use it in other steps.
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [create-kind-cluster]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-install.sh

  - name: test-backup-restore
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [test-install]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh
      - bats -t katalog/tests/velero/velero-file-system-backup.sh
      - bats -t katalog/tests/velero/velero-snapshot-backup.sh

  - name: init-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore]
    environment:
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${AWS_CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
      - terraform output -raw cloud_credentials > "$${AWS_CLOUD_CREDENTIALS_PATH}"
      - terraform output -raw backup_storage_location > "$${AWS_BACKUP_STORAGE_LOCATION_PATH}"

  - name: apply-aws-configuration
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [init-aws]
    commands:
      - |-
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kubectl apply -f "$${AWS_CLOUD_CREDENTIALS_PATH}" -n kube-system
      - kubectl apply -f "$${AWS_BACKUP_STORAGE_LOCATION_PATH}" -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - kubectl rollout status deploy/velero -n kube-system --timeout=180s
      - kubectl get pods -n kube-system

  - name: test-backup-restore-aws
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [apply-aws-configuration]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh

  - name: destroy-aws
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore-aws]
    environment:
      AWS_DEFAULT_REGION:
        from_secret: aws_region
      AWS_ACCESS_KEY_ID:
        from_secret: aws_access_key_id
      AWS_SECRET_ACCESS_KEY:
        from_secret: aws_secret_access_key
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: aws_terraform_tf_states_bucket_name
    commands:
      - cd examples/aws-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="key=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${AWS_CI_PIPELINE_NUMBER}"
        --backend-config="region=$${AWS_DEFAULT_REGION}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${CLUSTER_NAME}"
    when:
      status:
        - success
        - failure

  - name: init-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [destroy-aws]
    environment:
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - echo $${GCP_CREDENTIALS} > "$${GOOGLE_APPLICATION_CREDENTIALS}"
      - cd examples/gcp-example
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${GCP_CI_PIPELINE_NUMBER}"
      - terraform apply
        --auto-approve
        --var="my_cluster_name=$${GCP_CI_PIPELINE_NUMBER}-${DRONE_BUILD_NUMBER}"
        --var="gcp_project=$${GCP_PROJECT}"
      - terraform output -raw cloud_credentials > "$${GCP_CLOUD_CREDENTIALS_PATH}"
      - terraform output -raw backup_storage_location > "$${GCP_BACKUP_STORAGE_LOCATION_PATH}"

  - name: apply-gcp-configuration
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [init-gcp]
    commands:
      - |-
        mise use kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kubectl apply -f "$${GCP_CLOUD_CREDENTIALS_PATH}" -n kube-system
      - kubectl apply -f "$${GCP_BACKUP_STORAGE_LOCATION_PATH}" -n kube-system
      - kustomize build katalog/velero/velero-gcp | kubectl apply -f - -n kube-system
      - kubectl rollout restart deploy velero -n kube-system
      - kubectl rollout status deploy/velero -n kube-system --timeout=180s
      - kubectl get pods -n kube-system

  - name: test-backup-restore-gcp
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [apply-gcp-configuration]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-manifest-backup.sh

  - name: destroy-gcp
    image: hashicorp/terraform:1.4.6
    pull: always
    network_mode: host
    depends_on: [test-backup-restore-gcp]
    environment:
      GCP_PROJECT:
        from_secret: gcp_project
      GCP_CREDENTIALS:
        from_secret: gcp_credentials
      TERRAFORM_TF_STATES_BUCKET_NAME:
        from_secret: gcp_terraform_tf_states_bucket_name
    commands:
      - cd examples/gcp-example
      - echo $${GCP_CREDENTIALS} > $${GOOGLE_APPLICATION_CREDENTIALS}
      - terraform init
        --backend=true
        --backend-config="bucket=$${TERRAFORM_TF_STATES_BUCKET_NAME}"
        --backend-config="prefix=${DRONE_REPO_NAME}/${DRONE_BRANCH}/${DRONE_BUILD_NUMBER}/$${GCP_CI_PIPELINE_NUMBER}"
      - terraform destroy
        --auto-approve
        --var="my_cluster_name=$${GCP_CI_PIPELINE_NUMBER}-${DRONE_BUILD_NUMBER}"
        --var="gcp_project"=$${GCP_PROJECT}
    when:
      status:
        - success
        - failure

  - name: delete-kind-cluster
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [destroy-gcp]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - |
        mise use kind@$${KIND_VERSION}
        eval "$(mise activate bash --shims)"
      # does not matter if the command fails
      - kind delete cluster --name $${CLUSTER_NAME} || true
    when:
      status:
        - success
        - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock
  - name: mise-cache
    host:
      path: /root/mise_data_dir

# e2e-upgrade pipelines start here.
---
name: e2e-kubernetes-1.31-upgrade
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

environment:
  MISE_DATA_DIR: "/mise-data"
  MISE_OVERRIDE_CONFIG_FILENAMES: "mise.ci.toml"

  CLUSTER_VERSION: "v1.31.9"
  CLUSTER_NAME: "${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.31.9-upgrade"

  KUBECTL_VERSION: "v1.31.9"
  JQ_VERSION: "1.8.1"
  BATS_VERSION: "1.1.0"
  KUSTOMIZE_VERSION: "5.6.0"
  VELERO_VERSION: "1.17.1"

  KUBECONFIG: "/drone/src/kubeconfig-131-upgrade"

steps:
  - name: create-kind-cluster-upgrade
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [clone]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    environment:
      GITHUB_TOKEN:
        from_secret: github_token
    commands:
      - |-
        mise use kind@$${KIND_VERSION} kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install-previous-upgrade
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [create-kind-cluster-upgrade]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-install-previous.sh

  - name: test-upgrade-backup-restore
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [test-install-previous-upgrade]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-upgrade-file-system.sh

  - name: delete-kind-cluster-upgrade
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - |
        mise use kind@$${KIND_VERSION}
        eval "$(mise activate bash --shims)"
      - kind delete cluster --name $${CLUSTER_NAME} || true
    depends_on: [test-upgrade-backup-restore]
    when:
      status:
        - success
        - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock
  - name: mise-cache
    host:
      path: /root/mise_data_dir

---
name: e2e-kubernetes-1.32-upgrade
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

environment:
  MISE_DATA_DIR: "/mise-data"
  MISE_OVERRIDE_CONFIG_FILENAMES: "mise.ci.toml"

  CLUSTER_VERSION: "v1.32.5"
  CLUSTER_NAME: "${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.32.5-upgrade"

  KUBECTL_VERSION: "v1.32.5"
  JQ_VERSION: "1.8.1"
  BATS_VERSION: "1.1.0"
  KUSTOMIZE_VERSION: "5.6.0"
  VELERO_VERSION: "1.17.1"

  KUBECONFIG: "/drone/src/kubeconfig-132-upgrade"

steps:
  - name: create-kind-cluster-upgrade
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [clone]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    environment:
      GITHUB_TOKEN:
        from_secret: github_token
    commands:
      - |-
        mise use kind@$${KIND_VERSION} kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install-previous-upgrade
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [create-kind-cluster-upgrade]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-install-previous.sh

  - name: test-upgrade-backup-restore
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [test-install-previous-upgrade]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-upgrade-file-system.sh

  - name: delete-kind-cluster-upgrade
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - |
        mise use kind@$${KIND_VERSION}
        eval "$(mise activate bash --shims)"
      - kind delete cluster --name $${CLUSTER_NAME} || true
    depends_on: [test-upgrade-backup-restore]
    when:
      status:
        - success
        - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock
  - name: mise-cache
    host:
      path: /root/mise_data_dir

---
name: e2e-kubernetes-1.33-upgrade
kind: pipeline
type: docker

depends_on:
  - policeman

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**

environment:
  MISE_DATA_DIR: "/mise-data"
  MISE_OVERRIDE_CONFIG_FILENAMES: "mise.ci.toml"

  CLUSTER_VERSION: "v1.33.0"
  CLUSTER_NAME: "${DRONE_REPO_NAME}-${DRONE_BUILD_NUMBER}-1.33.0-upgrade"

  KUBECTL_VERSION: "v1.33.0"
  JQ_VERSION: "1.8.1"
  BATS_VERSION: "1.1.0"
  KUSTOMIZE_VERSION: "5.6.0"
  VELERO_VERSION: "1.17.1"

  KUBECONFIG: "/drone/src/kubeconfig-134-upgrade"

steps:
  - name: create-kind-cluster-upgrade
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    depends_on: [clone]
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    environment:
      GITHUB_TOKEN:
        from_secret: github_token
    commands:
      - |-
        mise use kind@$${KIND_VERSION} kubectl@$${KUBECTL_VERSION}
        eval "$(mise activate bash --shims)"
      - kind create cluster --name $${CLUSTER_NAME} --image registry.sighup.io/fury/kindest/node:$${CLUSTER_VERSION} --config katalog/tests/kind-config.yml
      - kind get kubeconfig --name $${CLUSTER_NAME} > $${KUBECONFIG}

  - name: test-install-previous-upgrade
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [create-kind-cluster-upgrade]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-install-previous.sh

  - name: test-upgrade-backup-restore
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    network_mode: host
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    depends_on: [test-install-previous-upgrade]
    commands:
      - |-
        mise use bats@$${BATS_VERSION} kubectl@$${KUBECTL_VERSION} \
          jq@$${JQ_VERSION} kustomize@$${KUSTOMIZE_VERSION} velero@$${VELERO_VERSION}
        eval "$(mise activate bash --shims)"
      - bats -t katalog/tests/velero/velero-upgrade-file-system.sh

  - name: delete-kind-cluster-upgrade
    image: quay.io/sighup/mise:v2025.4.4
    pull: always
    volumes:
      - name: dockersock
        path: /var/run/docker.sock
      - name: mise-cache
        path: /mise-data
    commands:
      - |
        mise use kind@$${KIND_VERSION}
        eval "$(mise activate bash --shims)"
      - kind delete cluster --name $${CLUSTER_NAME} || true
    depends_on: [test-upgrade-backup-restore]
    when:
      status:
        - success
        - failure

volumes:
  - name: dockersock
    host:
      path: /var/run/docker.sock
  - name: mise-cache
    host:
      path: /root/mise_data_dir


---
name: release
kind: pipeline
type: docker

depends_on:
  - e2e-kubernetes-1.31
  - e2e-kubernetes-1.32
  - e2e-kubernetes-1.33
  - e2e-kubernetes-1.34
  - e2e-kubernetes-1.31-upgrade
  - e2e-kubernetes-1.32-upgrade
  - e2e-kubernetes-1.33-upgrade
  # There's no upgrade for 1.33 because Velero 1.15.x didn't have support for 1.33. 

platform:
  os: linux
  arch: amd64

trigger:
  ref:
    include:
      - refs/tags/**
    exclude:
      - refs/tags/e2e-**

steps:
  - name: prepare-tar-gz
    image: alpine:latest
    pull: always
    depends_on: [clone]
    commands:
      - tar -zcvf fury-kubernetes-dr-${DRONE_TAG}.tar.gz katalog/ LICENSE README.md
    when:
      ref:
        include:
          - refs/tags/**

  - name: prepare-release-notes
    image: quay.io/sighup/fury-release-notes-plugin:3.7_2.8.4
    pull: always
    depends_on: [clone]
    settings:
      release_notes_file_path: release-notes.md
    when:
      ref:
        include:
          - refs/tags/**

  - name: publish-prerelease
    image: plugins/github-release
    pull: always
    depends_on:
      - prepare-tar-gz
      - prepare-release-notes
    settings:
      api_key:
        from_secret: github_token
      file_exists: overwrite
      files:
        - fury-kubernetes-dr-${DRONE_TAG}.tar.gz
      prerelease: true
      overwrite: true
      title: "Preview ${DRONE_TAG}"
      note: release-notes.md
      checksum:
        - md5
        - sha256
    when:
      ref:
        include:
          - refs/tags/v**-rc**

  - name: publish-stable
    image: plugins/github-release
    pull: always
    depends_on:
      - prepare-tar-gz
      - prepare-release-notes
    settings:
      api_key:
        from_secret: github_token
      file_exists: overwrite
      files:
        - fury-kubernetes-dr-${DRONE_TAG}.tar.gz
      prerelease: false
      overwrite: true
      title: "Release ${DRONE_TAG}"
      note: release-notes.md
      checksum:
        - md5
        - sha256
    when:
      ref:
        exclude:
          - refs/tags/v**-rc**
        include:
          - refs/tags/v**
